# Intent Classification Testing & Fine-Tuning System

## 📊 System Overview

A comprehensive testing and fine-tuning framework for the intent classification system with:

✅ **Automated Testing** - Run 50 test cases with a single command
✅ **Performance Metrics** - Accuracy, Precision, Recall, F1, Confusion Matrix
✅ **Misclassification Analysis** - Identify patterns and problem areas
✅ **Fine-Tuning Pipeline** - Generate OpenAI fine-tuning datasets automatically
✅ **Continuous Improvement** - Iterative model refinement workflow

## 🚀 Quick Start

### Run Tests
```bash
cd /Users/markwang/Documents/Dispatch\ AI/backend/ai
./app/intent_classification/tests/run_tests.sh
```

### Run Demo (3 test cases)
```bash
.venv/bin/python app/intent_classification/tests/demo.py
```

## 📁 Files Created

### Core Files
| File | Purpose | Lines |
|------|---------|-------|
| `test_runner.py` | Main test runner with metrics calculation | ~380 |
| `fine_tuning.py` | Fine-tuning data generator | ~320 |
| `demo.py` | Quick demo script | ~80 |

### Documentation
| File | Purpose |
|------|---------|
| `README.md` | Complete testing documentation |
| `QUICK_START.md` | Quick reference guide |
| `TESTING_SUMMARY.md` | This file |

### Scripts
| File | Purpose |
|------|---------|
| `run_tests.sh` | Single command to run everything |

### Directories
| Directory | Purpose |
|-----------|---------|
| `results/` | Test metrics, reports, misclassifications |
| `fine_tuning_data/` | Generated fine-tuning datasets (JSONL) |

## 📈 Metrics Calculated

### Overall Metrics
- **Accuracy**: Percentage of correct classifications
- **Macro Precision**: Average precision across all intents
- **Macro Recall**: Average recall across all intents
- **Macro F1**: Average F1 score across all intents

### Per-Intent Metrics (SCAM, OPPORTUNITY, OTHER)
- **Precision**: TP / (TP + FP)
- **Recall**: TP / (TP + FN)
- **F1 Score**: Harmonic mean of precision and recall
- **Support**: Total number of test cases
- **Average Confidence**: Mean confidence score
- **Confusion Matrix**: Intent x Intent classification matrix

### Analysis Features
- Misclassification patterns (e.g., "scam → opportunity: 3 cases")
- Low confidence cases identification
- Detailed case-by-case breakdown
- Keyword and characteristic matching analysis

## 🔧 Fine-Tuning Workflow

### 1. Run Tests
```bash
./app/intent_classification/tests/run_tests.sh
```
Output: `results/metrics_YYYYMMDD_HHMMSS.json`

### 2. Generate Fine-Tuning Data
Automatically generated by `run_tests.sh` or manually:
```bash
.venv/bin/python app/intent_classification/tests/fine_tuning.py \
  app/intent_classification/tests/results/metrics_YYYYMMDD_HHMMSS.json
```
Output:
- `fine_tuning_data/train_YYYYMMDD_HHMMSS.jsonl`
- `fine_tuning_data/validation_YYYYMMDD_HHMMSS.jsonl`

### 3. Upload to OpenAI
```bash
openai api files.create -f fine_tuning_data/train_*.jsonl -p fine-tune
openai api files.create -f fine_tuning_data/validation_*.jsonl -p fine-tune
```

### 4. Create Fine-Tuning Job
```bash
openai api fine_tunes.create \
  -t file-TRAIN_ID \
  -v file-VAL_ID \
  -m gpt-3.5-turbo \
  --suffix "intent-classification"
```

### 5. Monitor Progress
```bash
openai api fine_tunes.follow -i ft-JOB_ID
```

### 6. Deploy Fine-Tuned Model
```python
# Update settings
OPENAI_MODEL=ft:gpt-3.5-turbo:org:intent-classification:JOB_ID
```

### 7. Validate Improvements
```bash
./app/intent_classification/tests/run_tests.sh
```
Compare new metrics to baseline.

## 📊 Output Examples

### Test Report Sample
```
============================================================
INTENT CLASSIFICATION TEST REPORT
============================================================

OVERALL PERFORMANCE
Total Tests:    50
Correct:        47 (94.0%)
Accuracy:       94.0%
Macro Precision: 93.5%
Macro Recall:    94.2%
Macro F1:        93.8%

PER-INTENT PERFORMANCE

SCAM:
  Precision:      100.0%
  Recall:         93.3%
  F1 Score:       96.5%
  Support:        15 cases
  Avg Confidence: 0.94

CONFUSION MATRIX
Actual/Predicted    scam           opportunity    other
scam                14             1              0
opportunity         0              14             1
other               0              1              14
```

### Fine-Tuning Data Sample (JSONL)
```json
{
  "messages": [
    {
      "role": "system",
      "content": "You are an intent classification system..."
    },
    {
      "role": "user",
      "content": "CONVERSATION CONTEXT:\n\nCurrent Student Message: We'd like to invite you for a job interview..."
    },
    {
      "role": "assistant",
      "content": "{\"intent\": \"opportunity\", \"confidence\": 0.95, \"reasoning\": \"Legitimate job interview invitation\", \"metadata\": {...}}"
    }
  ]
}
```

## 🎯 Success Criteria

| Metric | Excellent | Good | Needs Work |
|--------|-----------|------|------------|
| **Overall Accuracy** | > 95% | 90-95% | < 90% |
| **SCAM Recall** | 100% | 95-99% | < 95% ⚠️ |
| **OPPORTUNITY Precision** | > 90% | 85-90% | < 85% |
| **F1 Score (all intents)** | > 0.90 | 0.85-0.90 | < 0.85 |

**Critical**: SCAM recall < 95% is dangerous - means missing fraud attempts!

## 🔄 Continuous Improvement Cycle

```
1. Baseline Testing
   ↓
2. Identify Weak Areas (Confusion Matrix, Misclassifications)
   ↓
3. Generate Fine-Tuning Data (Emphasize weak areas)
   ↓
4. Train Model on OpenAI
   ↓
5. Deploy Fine-Tuned Model
   ↓
6. Validate Improvements
   ↓
7. Repeat → Continuous Improvement
```

## 🧪 Test Coverage

### Test Data Statistics
- **SCAM**: 15 test cases (fraud scenarios)
- **OPPORTUNITY**: 15 test cases (jobs, research, internships)
- **OTHER**: 15 test cases (messages, callbacks, complex)
- **Edge Cases**: 5 test cases (ambiguous boundaries)
- **Total**: 50 test cases

### Fine-Tuning Data Generation
- **Base Examples**: 50 (from test cases)
- **Correction Examples**: Variable (from misclassifications)
- **Augmentation**: 3x for misclassified cases
- **Train/Val Split**: 80/20

## 🛠️ Technical Details

### Dependencies
- Python 3.8+
- OpenAI Python SDK
- asyncio (async classification)
- JSON for data export
- Collections for metrics

### Integration Points
- Works with existing `IntentClassifier`
- Uses existing test data structure
- Compatible with OpenAI fine-tuning API
- Outputs standard JSONL format

### Performance
- Test Speed: ~2-3 seconds per case (API latency)
- Total Time: ~2-3 minutes for 50 cases
- Metrics Calculation: <1 second
- Fine-Tuning Data Generation: <5 seconds

## 📝 Key Features

### Test Runner (`test_runner.py`)
✅ Async test execution
✅ Real-time progress tracking
✅ Comprehensive metrics calculation
✅ Confusion matrix generation
✅ Misclassification pattern analysis
✅ JSON + Text report output
✅ Automatic results saving

### Fine-Tuning Generator (`fine_tuning.py`)
✅ OpenAI chat format compatibility
✅ Base test case conversion
✅ Misclassification correction examples
✅ Data augmentation (3x for corrections)
✅ Train/validation split (80/20)
✅ JSONL export for OpenAI
✅ Improvement analysis report

### Documentation
✅ Comprehensive README (300+ lines)
✅ Quick start guide
✅ Code examples
✅ Troubleshooting tips
✅ Best practices
✅ Step-by-step workflows

## 🎓 Usage Examples

### Programmatic Access
```python
from intent_classification.tests.test_runner import IntentTestRunner

runner = IntentTestRunner()
metrics = await runner.run_all_tests()

print(f"Accuracy: {metrics['summary']['accuracy']:.1%}")
print(f"F1 Score: {metrics['summary']['macro_f1']:.1%}")

for case in metrics['misclassified_cases']:
    print(f"Failed: {case['test_id']}")
```

### Custom Test Suite
```python
custom_tests = {
    "scam": SCAM_TEST_CASES[:5],  # First 5 only
    "opportunity": OPPORTUNITY_TEST_CASES
}

metrics = await runner.run_all_tests(custom_tests)
```

### Fine-Tuning Data Generation
```python
from intent_classification.tests.fine_tuning import FineTuningDataGenerator

generator = FineTuningDataGenerator()
examples = generator.generate_from_test_cases()
train, val = generator.create_validation_split(examples)
generator.save_fine_tuning_dataset(train, name="custom_train")
```

## 📞 Support

For questions or issues:
1. Check `README.md` for detailed documentation
2. Check `QUICK_START.md` for common tasks
3. Review test results for error messages
4. Check logs in `results/` directory
5. Contact development team

## 🔗 Related Files

- Main classifier: `app/intent_classification/services/classifier.py`
- Test data: `app/intent_classification/tests/test_data/*.py`
- Intent definitions: `app/intent_classification/definitions/*.py`
- API routes: `app/intent_classification/api/routes.py`

---

**Version**: 2.0.0
**Created**: 2025-10-22
**Last Updated**: 2025-10-22
**Author**: Dispatch AI Development Team
